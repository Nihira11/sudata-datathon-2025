---
title: "Modelling"
author: "Nihira"
date: "2025-10-01"
output: html_document
---

```{r}
library(tidyverse)
library(dplyr)
library(janitor)
library(tibble)
library(caret)
library(vip)
library(kableExtra)
library(randomForest)
library(xgboost)
library(nnet)
library(dendextend)
library(factoextra)
library(RColorBrewer)
library(doParallel)
library(smotefamily)
library(corrplot)
library(MASS)

set.seed(123)
```

```{r}
df <- read_csv("~/Desktop/sudata-datathon-2025/datasets/dynamic_supply_chain_logistics_dataset.csv") %>%
  clean_names()

glimpse(df)
```

```{r}
# Data Prep
target_cols <- c("disruption_likelihood_score",
                 "delay_probability",
                 "risk_classification",
                 "delivery_time_deviation")

df_mod <- df |> drop_na(all_of(target_cols))

drop_cols <- c(target_cols,
               "timestamp",
               "vehicle_gps_latitude",
               "vehicle_gps_longitude")

predictor_cols <- df_mod %>%
  dplyr::select(-any_of(drop_cols)) %>%
  dplyr::select(where(is.numeric)) %>%
  names()

X <- df_mod |> dplyr::select(all_of(predictor_cols))
```

```{r}
X_scaled <- scale(X)
d <- dist(t(X_scaled), method = "euclidean")
hc <- hclust(d, method = "ward.D2")
```

```{r}
clust_4 <- cutree(hc, k = 4)
groups <- split(names(clust_4), clust_4)
groups
```

```{r}
groups <- list(
  Operations = c("fuel_consumption_rate", "loading_unloading_time",
          "handling_equipment_availability", "port_congestion_level",
          "supplier_reliability_score", "iot_temperature",
          "route_risk_level", "customs_clearance_time"),

  Risk = c("eta_variation_hours", "traffic_congestion_level",
           "weather_condition_severity", "driver_behavior_score",
           "fatigue_monitoring_score"),

  Supply = c("warehouse_inventory_level", "lead_time_days"),

  Logistics = c("order_fulfillment_status", "shipping_costs",
                "historical_demand", "cargo_condition_status")
)
```

```{r}
# Train/Test Split
df_mod <- df |>
  dplyr::mutate(risk_classification = factor(risk_classification)) |>
  tidyr::drop_na(dplyr::all_of(target_cols))

drop_cols <- c(target_cols, "timestamp", "vehicle_gps_latitude", "vehicle_gps_longitude")
X <- df_mod |>
  dplyr::select(-dplyr::any_of(drop_cols)) |>
  dplyr::select(where(is.numeric))

idx <- caret::createDataPartition(df_mod$risk_classification, p = 0.8, list = FALSE)
train <- df_mod[idx, ]
test <- df_mod[-idx, ]
Xtr <- X[idx, ]
Xte  <- X[-idx, ]
```

```{r}
fit_group_pca <- function(train_df, test_df, groups, var_exp = 0.90) {
  train_out <- list(); test_out <- list(); store <- list()
  for (g in names(groups)) {
    cols <- groups[[g]]
    x_tr <- scale(train_df[, cols, drop = FALSE])
    x_te <- scale(test_df[, cols, drop = FALSE],
                  center = attr(x_tr, "scaled:center"),
                  scale  = attr(x_tr, "scaled:scale"))
    pca <- prcomp(x_tr, center = FALSE, scale. = FALSE)
    vr  <- pca$sdev^2 / sum(pca$sdev^2)
    k   <- max(1, which(cumsum(vr) >= var_exp)[1])
    tr_scores <- as.data.frame(pca$x[, 1:k, drop = FALSE])
    te_scores <- as.data.frame(as.matrix(x_te) %*% pca$rotation[, 1:k, drop = FALSE])
    names(tr_scores) <- paste0(g, "_PC", seq_len(k))
    names(te_scores) <- names(tr_scores)
    train_out[[g]] <- tr_scores; test_out[[g]] <- te_scores; store[[g]] <- list(pca=pca,k=k)
  }
  list(Xtr = dplyr::bind_cols(train_out), Xte = dplyr::bind_cols(test_out), store = store)
}

gp  <- fit_group_pca(Xtr, Xte, groups, var_exp = 0.90)
GXtr <- gp$Xtr
GXte <- gp$Xte
```

```{r}
# Classification
train_cls_noleak <- bind_cols(GXtr, risk_classification = train$risk_classification) %>%
  dplyr::mutate(risk_classification = factor(make.names(risk_classification)))

test_cls_noleak  <- bind_cols(GXte, risk_classification = test$risk_classification) %>%
  dplyr::mutate(risk_classification = factor(make.names(risk_classification),
                                             levels = levels(train_cls_noleak$risk_classification)))

# --- Regression Targets ---
train_dis_noleak <- bind_cols(GXtr, y = train$disruption_likelihood_score)
test_dis_noleak  <- bind_cols(GXte,  y = test$disruption_likelihood_score)

train_dp_noleak  <- bind_cols(GXtr, y = train$delay_probability)
test_dp_noleak   <- bind_cols(GXte,  y = test$delay_probability)

train_dev_noleak <- bind_cols(GXtr, y = train$delivery_time_deviation)
test_dev_noleak  <- bind_cols(GXte,  y = test$delivery_time_deviation)
```

```{r}
suppressWarnings({
  if ("package:doParallel" %in% search()) foreach::registerDoSEQ()
})

ctrl_cv3 <- trainControl(method = "cv", number = 3, classProbs = TRUE, allowParallel = FALSE)

xgb_grid <- expand.grid(
  nrounds = 100,
  max_depth = c(3, 5),
  eta = 0.1,
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)
```

```{r}
# Risk Classification
train_cls_noleak$risk_classification <- factor(make.names(train_cls_noleak$risk_classification))
test_cls_noleak$risk_classification  <- factor(make.names(test_cls_noleak$risk_classification),
                                               levels = levels(train_cls_noleak$risk_classification))

ctrl_logit <- trainControl(
  method = "cv",
  number = 3,
  sampling = "smote",
  classProbs = TRUE,
  allowParallel = FALSE
)

set.seed(123)
model_logit <- train(
  risk_classification ~ ., 
  data = train_cls_noleak,
  method = "multinom",
  trControl = ctrl_logit,
  trace = FALSE
)

pred_logit <- predict(model_logit, newdata = test_cls_noleak)
cm_logit <- confusionMatrix(pred_logit, test_cls_noleak$risk_classification)
print(cm_logit)
```

```{r}
# XGBoost
grid <- expand.grid(
  nrounds = 100,
  max_depth = c(3, 5),
  eta = 0.1,
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

ctrl_xgb <- trainControl(
  method = "cv",
  number = 3,
  sampling = "smote",
  classProbs = TRUE,
  allowParallel = FALSE
)

set.seed(123)
model_xgb <- train(
  risk_classification ~ ., 
  data = train_cls_noleak,
  method = "xgbTree",
  trControl = ctrl_xgb,
  tuneGrid = grid
)

pred_xgb <- predict(model_xgb, newdata = test_cls_noleak)
cm_xgb <- confusionMatrix(pred_xgb, test_cls_noleak$risk_classification)
print(cm_xgb)
```

```{r}
imp <- xgb.importance(model = model_xgb$finalModel)

ggplot(head(imp, 12), aes(x = reorder(Feature, Gain), y = Gain, fill = Gain)) +
  geom_col() +
  geom_text(aes(label = round(Gain, 3)), hjust = 1.1, color = "white", size = 3.5) + 
  coord_flip() +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen") +
  theme_minimal(base_size = 14) +
  labs(
    title = "XGBoost Feature Importance (Top 12 Predictors)",
    x = "Feature",
    y = "Importance (Gain)"
  )
```

```{r}
grid_reg <- expand.grid(
  nrounds = 100,
  max_depth = c(3, 5),
  eta = 0.1,
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

ctrl_reg <- trainControl(
  method = "cv",
  number = 3,
  allowParallel = FALSE
)
```

```{r}
# Disruption Likelihood Score

# Linear Regression (baseline)
train_idx <- createDataPartition(df_mod$disruption_likelihood_score, p = 0.7, list = FALSE)
train_reg <- df_mod[train_idx, ]
test_reg  <- df_mod[-train_idx, ]

model_lm <- train(
  disruption_likelihood_score ~ .,
  data = train_reg[, c(predictor_cols, "disruption_likelihood_score")],
  method = "lm",
  trControl = trainControl(method = "cv", number = 3)
)

pred_lm <- predict(model_lm, newdata = test_reg)
postResample(pred_lm, test_reg$disruption_likelihood_score)
```

```{r}
# XGBoost
train_dis_grp <- bind_cols(GXtr, y = train$disruption_likelihood_score)
test_dis_grp  <- bind_cols(GXte,  y = test$disruption_likelihood_score)

model_xgb_reg <- train(
  y ~ ., data = train_dis_grp,
  method = "xgbTree",
  trControl = ctrl_reg,
  tuneGrid = grid_reg
)

pred_xgb_reg <- predict(model_xgb_reg, newdata = test_dis_grp)
postResample(pred_xgb_reg, test_dis_grp$y)
```

```{r}
imp_reg <- xgb.importance(model = model_xgb_reg$finalModel)

ggplot(head(imp_reg, 12), aes(x = reorder(Feature, Gain), y = Gain, fill = Gain)) +
  geom_col() +
  geom_text(aes(label = round(Gain, 3)), hjust = 1.1, color = "white", size = 3.5) +
  coord_flip() +
  scale_fill_gradient(low = "gold", high = "brown") +
  theme_minimal(base_size = 14) +
  labs(title = "XGBoost Group Importance – Disruption Likelihood",
       x = "Group Feature", y = "Importance (Gain)")
```

```{r}
# Delay Probability

# Linear Regression (baseline)
train_idx <- createDataPartition(df_mod$delay_probability, p = 0.7, list = FALSE)
train_dp <- df_mod[train_idx, ]
test_dp  <- df_mod[-train_idx, ]

model_lm_dp <- train(
  delay_probability ~ .,
  data = train_dp[, c(predictor_cols, "delay_probability")],
  method = "lm",
  trControl = trainControl(method = "cv", number = 3)
)

pred_lm_dp <- predict(model_lm_dp, newdata = test_dp)
postResample(pred_lm_dp, test_dp$delay_probability)
```

```{r}
# XGBoost
train_dp_grp <- bind_cols(GXtr, y = train$delay_probability)
test_dp_grp  <- bind_cols(GXte,  y = test$delay_probability)

model_xgb_dp <- train(
  y ~ ., data = train_dp_grp,
  method = "xgbTree",
  trControl = ctrl_reg,
  tuneGrid = grid_reg
)

pred_xgb_dp <- predict(model_xgb_dp, newdata = test_dp_grp)
postResample(pred_xgb_dp, test_dp_grp$y)
```

```{r}
imp_dp <- xgb.importance(model = model_xgb_dp$finalModel)

ggplot(head(imp_dp, 12), aes(x = reorder(Feature, Gain), y = Gain, fill = Gain)) +
  geom_col() +
  geom_text(aes(label = round(Gain, 3)), hjust = 1.1, color = "white", size = 3.5) +
  coord_flip() +
  scale_fill_gradient(low = "skyblue", high = "navy") +
  theme_minimal(base_size = 14) +
  labs(title = "XGBoost Group Importance – Delay Probability",
       x = "Group Feature", y = "Importance (Gain)")
```

```{r}
# Delivery Time Deviation

# Linear Regression (baseline)
train_idx <- createDataPartition(df_mod$delivery_time_deviation, p = 0.7, list = FALSE)
train_dev <- df_mod[train_idx, ]
test_dev  <- df_mod[-train_idx, ]

model_lm_dev <- train(
  delivery_time_deviation ~ .,
  data = train_dev[, c(predictor_cols, "delivery_time_deviation")],
  method = "lm",
  trControl = trainControl(method = "cv", number = 3)
)

pred_lm_dev <- predict(model_lm_dev, newdata = test_dev)
postResample(pred_lm_dev, test_dev$delivery_time_deviation)
```

```{r}
# XGBoost
train_dev_grp <- bind_cols(GXtr, y = train$delivery_time_deviation)
test_dev_grp  <- bind_cols(GXte,  y = test$delivery_time_deviation)

model_xgb_dev <- train(
  y ~ ., data = train_dev_grp,
  method = "xgbTree",
  trControl = ctrl_reg,
  tuneGrid = grid_reg
)

pred_xgb_dev <- predict(model_xgb_dev, newdata = test_dev_grp)
postResample(pred_xgb_dev, test_dev_grp$y)
```

```{r}
imp_dev <- xgb.importance(model = model_xgb_dev$finalModel)

ggplot(head(imp_dev, 12), aes(x = reorder(Feature, Gain), y = Gain, fill = Gain)) +
  geom_col() +
  geom_text(aes(label = round(Gain, 3)), hjust = 1.1, color = "white", size = 3.5) +
  coord_flip() +
  scale_fill_gradient(low = "lightcoral", high = "darkred") +
  theme_minimal(base_size = 14) +
  labs(title = "XGBoost Group Importance – Delivery Time Deviation",
       x = "Group Feature", y = "Importance (Gain)")
```


